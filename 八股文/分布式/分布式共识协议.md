#### Paxos
Paxos的最大特点就是难，不仅难以理解，更难以实现。

Paxos算法运行在允许宕机故障的异步系统中，不要求可靠的消息传递，可容忍消息丢失、延迟、乱序以及重复。它利用大多数 (Majority) 机制保证了2F+1的容错能力，**即2F+1个节点的系统最多允许F个节点同时出现故障**。

一个或多个提议进程 (Proposer) 可以发起提案 (Proposal)，Paxos算法使所有提案中的某一个提案，在所有进程中达成一致。系统中的多数派同时认可该提案，即达成了一致。最多只针对一个确定的提案达成一致。

Paxos将系统中的角色分为提议者 (Proposer)，决策者 (Acceptor)，和最终决策学习者 (Learner):

- **Proposer**: 提出提案 (Proposal)。Proposal信息包括**提案编号 (Proposal ID) 和提议的值 (Value)**。
- **Acceptor**：参与决策，回应Proposers的提案。收到Proposal后可以接受提案，若Proposal获得多数Acceptors的接受，则称该Proposal被批准。
- **Learner**：不参与决策，从Proposers/Acceptors学习最新达成一致的提案（Value）
![](https://pic3.zhimg.com/v2-2c0d971fcca713a8e045a93d7881aedc_1440w.jpg)

Paxos算法通过一个决议分为两个阶段（Learn阶段之前决议已经形成）：

1. 第一阶段：Prepare阶段。Proposer向Acceptors发出Prepare请求，Acceptors针对收到的Prepare请求进行Promise承诺。
2. 第二阶段：Accept阶段。Proposer收到多数Acceptors承诺的Promise后，向Acceptors发出Propose请求，Acceptors针对收到的Propose请求进行Accept处理。
3. 第三阶段：Learn阶段。Proposer在收到多数Acceptors的Accept之后，标志着本次Accept成功，决议形成，将形成的决议发送给所有Learners。
![](https://pic2.zhimg.com/v2-a6cd35d4045134b703f9d125b1ce9671_1440w.jpg)

两个承诺：

1. 不再接受Proposal ID小于等于（注意：这里是<= ）当前请求的Prepare请求。
2. 不再接受Proposal ID小于（注意：这里是< ）当前请求的Propose请求。

一个应答：

不违背以前作出的承诺下，回复已经Accept过的提案中Proposal ID最大的那个提案的Value和Proposal ID，没有则返回空值。

- **Propose**: Proposer 收到多数Acceptors的Promise应答后，从应答中选择Proposal ID最大的提案的Value，作为本次要发起的提案。如果所有应答的提案Value均为空值，则可以自己随意决定提案Value。然后携带当前Proposal ID，向所有Acceptors发送Propose请求。
- **Accept**: Acceptor收到Propose请求后，在不违背自己之前作出的承诺下，接受并持久化当前Proposal ID和提案Value。
- **Learn**: Proposer收到多数Acceptors的Accept后，决议形成，将形成的决议发送给所有Learners。

实际应用中几乎都需要连续确定多个值，而且希望能有更高的效率。Multi-Paxos正是为解决此问题而提出。Multi-Paxos基于Basic Paxos做了两点改进：

1. 针对每一个要确定的值，运行一次Paxos算法实例（Instance），形成决议。**每一个Paxos实例使用唯一的Instance ID标识**。
2. 在所有Proposers中**选举一个Leader，由Leader唯一地提交Proposal给Acceptors进行表决**。这样没有Proposer竞争，解决了活锁问题。在系统中仅有一个Leader进行Value提交的情况下，Prepare阶段就可以跳过，从而将两阶段变为一阶段，提高效率。
![](https://pic1.zhimg.com/v2-e5cd197abc9c922ca4ca91c3df74fa70_1440w.jpg)

Multi-Paxos首先需要选举Leader，Leader的确定也是一次决议的形成，所以可执行一次Basic Paxos实例来选举出一个Leader。选出Leader之后只能由Leader提交Proposal，在Leader宕机之后服务临时不可用，需要重新选举Leader继续服务。在系统中仅有一个Leader进行Proposal提交的情况下，Prepare阶段可以跳过。

Multi-Paxos通过改变Prepare阶段的作用范围至后面Leader提交的所有实例，从而使得Leader的连续提交**只需要执行一次Prepare阶段**，后续只需要执行Accept阶段，将两阶段变为一阶段，提高了效率。为了区分连续提交的多个实例，每个实例使用一个Instance ID标识，Instance ID由Leader本地递增生成即可。

最后，我来用一个实例去解释Paxos
首先，要传递老师的一段话，但是当时同学们没有电话，只能口头传达
老师把话传给一些有威望的同学Proposer，让他们给全班通知
老师的话会有时间戳，传递的时候会带上时间，比如"在今天五点的时候，老师说明天开班会"
一些被通知的同学Acceptor，接收到Proposer提供的信息，回复他们了解了
并且承诺Promise不会再听在这个信息之前的旧信息
老师突然想改口，于是在六点给一部分Proposer传递了新消息
被通知的同学又被Proposer通知，看到是六点讲的话，于是就抛弃五点的信息，接收六点的
如果是传递旧信息的Proposer来，那就会回复他有新信息了，那这个Proposer的信息变为新的
Proposer看到大多数同学都知道这个最新信息了，于是和Acceptor一起通知所有同学Learner
但是因为太多Proposer了，信息的更新变得非常复杂
所以选出一个班长Leader，只给他传递信息，就不会出现旧信息的情况

#### Raft
不同于Paxos算法直接从分布式一致性问题出发推导出来，Raft算法则是从多副本状态机的角度提出，用于管理多副本状态机的日志复制。Raft实现了和Paxos相同的功能，它将一致性分解为多个子问题：Leader选举（Leader election）、日志同步（Log replication）、[安全性](https://zhida.zhihu.com/search?content_id=5010031&content_type=Article&match_order=1&q=%E5%AE%89%E5%85%A8%E6%80%A7&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDg0ODI2MTcsInEiOiLlronlhajmgKciLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjo1MDEwMDMxLCJjb250ZW50X3R5cGUiOiJBcnRpY2xlIiwibWF0Y2hfb3JkZXIiOjEsInpkX3Rva2VuIjpudWxsfQ.bhFhMRWg34inCeb7P-hJL3rJyFjlad9sX2kFFDZNnNg&zhida_source=entity)（Safety）、[日志压缩](https://zhida.zhihu.com/search?content_id=5010031&content_type=Article&match_order=1&q=%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9&zd_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJ6aGlkYV9zZXJ2ZXIiLCJleHAiOjE3NDg0ODI2MTcsInEiOiLml6Xlv5fljovnvKkiLCJ6aGlkYV9zb3VyY2UiOiJlbnRpdHkiLCJjb250ZW50X2lkIjo1MDEwMDMxLCJjb250ZW50X3R5cGUiOiJBcnRpY2xlIiwibWF0Y2hfb3JkZXIiOjEsInpkX3Rva2VuIjpudWxsfQ.2UDz7u-jjGeb7Vyh7BS8tdsz_aObjFuoU3qUDF2LF28&zhida_source=entity)（Log compaction）、成员变更（Membership change）等。同时，Raft算法使用了更强的假设来减少了需要考虑的状态，使之变的易于理解和实现。

Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选人（Candidate）：

- **Leader**：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。
- **Follower**：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。
- **Candidate**：Leader选举过程中的临时角色，可能转变成Leader。

Raft要求系统在任意时刻最多只有一个Leader，正常工作期间只有Leader和Followers。
Raft算法角色状态转换如下：
![](https://picx.zhimg.com/v2-7f64a2df8f8817932ed047d35878bca9_1440w.jpg)

Follower只响应其他服务器的请求。如果Follower超时没有收到Leader的消息，它会成为一个Candidate并且开始一次Leader选举。收到大多数服务器投票的Candidate会成为新的Leader。Leader在宕机之前会一直保持Leader的状态。
![](https://pica.zhimg.com/v2-d3cc1cb525ac72dc59ed34148cb3199c_1440w.jpg)
Raft算法将时间分为一个个的任期（term），每一个term的开始都是Leader选举。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束。

##### Leader选举
Raft 使用心跳（heartbeat）触发Leader选举。当服务器启动时，初始化为Follower。Leader向所有Followers周期性发送heartbeat。如果Follower在选举超时时间内没有收到Leader的heartbeat，或者Leader的任期到期了，就会等待一段随机的时间后发起一次Leader选举。

Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送拉票请求。结果有以下三种情况：
- 赢得了多数的选票，成功选举为Leader；
- 收到了Leader的消息，表示有其它服务器已经抢先当选了Leader；
- 没有服务器赢得多数的选票，Leader选举失败，等待选举时间超时后发起下一次选举。
![](https://pic2.zhimg.com/v2-0471619d1b78ba6d57326d97825d9495_1440w.jpg)
投票的时候，支持改投，如果Candidate拉票的时候看到其他的Candidate的term_id更大，就会把自己的那一票改投给它而不是自己

选举出Leader后，Leader通过定期向所有Followers发送心跳信息维持其统治。若Follower一段时间未收到Leader的心跳则认为Leader可能已经挂了，再次发起Leader选举过程

##### 日志同步
Leader选出后，就开始接收客户端的请求。Leader把请求作为日志条目（Log entries）加入到它的日志中，然后并行的向其他服务器发起 AppendEntries RPC复制日志条目。当这条日志被复制到大多数服务器上，Leader将这条日志应用到它的状态机并向客户端返回执行结果。
![](https://pic1.zhimg.com/v2-7cdaa12c6f34b1e92ef86b99c3bdcf32_1440w.jpg)
某些Followers可能没有成功的复制日志，Leader会无限的重试 AppendEntries RPC直到所有的Followers最终存储了所有的日志条目。

日志由有序编号（log index）的日志条目组成。每个日志条目包含它被创建时的任期号（term），和用于状态机执行的命令。如果一个日志条目被复制到大多数服务器上，就被认为可以提交（commit）了。
![](https://pic3.zhimg.com/v2-ee29a89e4eb63468e142bb6103dbe4de_1440w.jpg)

我来举个实例
首先，我们要选班长Leader
班长必须学到的知识是最新的
每个同学Candidate都很积极，报名参加了选举，他们手中都会有一票
选举开始的时候，他们先把那一票投给自己，然后拉票
拉票的时候，发现有人的知识比自己更新，就把那一票投给他，以后其他人拉票也不会改
直到有人的票超过班级人数的一半，他就成为班长了
然后选举结束，每个人都服从班长
班长每次颁布的政策，都会加上对应的任期时间
如果班长有事休学或者任期过了，那重新开始选举活动
每个同学参加选举的时间可能不同，避免所有同学同一时间参加，导致流程复杂
班长负责听从老师的要求，并传递给每个同学
一些同学可能在睡觉不能收到信息，班长会一直通知，直到他们收到
一半以上同学都了解了，那么就认为老师的要求成功传达了

#### Zab
ZAB协议是为分布式协调服务ZooKeeper专门设计的一种**支持崩溃恢复**的**原子广播协议**。

ZooKeeper使用一个单一的主进程来接受并处理客户端的所有事物请求，并采用ZAB的原子广播协议，将服务器数据的状态变更以事物Proposal的形式广播到所有的副本进程上去。

Zab分为消息广播和崩溃恢复两个阶段

##### 消息广播
Leader接收信息后，首先会询问所有Follower，一半以上同意后，则会推送信息，也给自己提交

- Leader 在收到客户端请求之后，会将这个请求封装成一个事务，并给这个事务分配一个全局递增的唯一 ID，称为**事务ID（ZXID）**，ZAB 协议需要保证事务的顺序，因此必须将每一个事务按照 ZXID 进行先后排序然后处理。
- 在 Leader 和 Follwer 之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞。
- zookeeper集群中为保证任何所有进程能够有序的顺序执行，只能是 Leader 服务器接受写请求，即使是 Follower 服务器接受到客户端的请求，也会转发到 Leader 服务器进行处理。
- 实际上，这是一种简化版本的 2PC，不能解决单点问题。等会我们会讲述 ZAB 如何解决单点问题（即 Leader 崩溃问题）。

##### 崩溃恢复
刚刚我们说消息广播过程中，Leader 崩溃怎么办？还能保证数据一致吗？如果 Leader 先本地提交了，然后 commit 请求没有发送出去，怎么办

- ZAB 协议确保那些已经在 Leader 提交的事务最终会被所有服务器提交。
- ZAB 协议确保丢弃那些只在 Leader 提出/复制，但没有提交的事务。

所以，ZAB 设计了下面这样一个选举算法：**能够确保提交已经被 Leader 提交的事务，同时丢弃已经被跳过的事务**。

针对这个要求，如果让 Leader 选举算法能够保证新选举出来的 Leader 服务器拥有集群总所有机器编号（即 ZXID 最大）的事务，那么就能够保证这个新选举出来的 Leader 一定具有所有已经提交的提案。

每次重新开始选举的时候，每个节点都会拉票，节点只会给ZXID最大的节点投票

举个例子
要选举班长，需要知识最新的人当班长
每个人去跟其他同学拉票，如果发现这个同学知识更新，就给他投票
直到有同学获得一半以上的票，则晋升为班长
只有班长休学的时候，才能重新选举
班长接收老师的要求，先跟所有同学说明有新要求
一半以上同学收到，班长才推送信息，并给自己提交这个信息